{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konvolučné neurónové siete - LeNet\n",
    "\n",
    "Na dnešnom cvičení naimplementujeme jeden zo základných konvolučných neurónových sietí, konkrétne LeNet. Naše riešenie budeme aplikovať na rozpoznávanie rukou písaných číslic z datasetu MNIST. LeNet bol pôvodne navrhnutý v roku 1989 pre rozpoznávanie číslic amerických PSČ. Sieť následne prešla niekoľkými iteráciami a v roku 1998 bol [publikovaný model lenet-5](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791), ktorý bol presnejší ako bežné modely počítačového videnia na rozpoznávanie rukou písaných číslic. LeNet bol takisto medzi prvými konvolučnými sieťami, ktoré používali algoritmus spätného šírenia chyby na trénovanie. V našom riešení urobíme niekoľko zmien oproti pôvodnému článku, najmä čo sa týka použitej aktivačnej funkcie (pôvodne `tanh`) a optimalizátora (pôvodne jednoduchý `SGD`).\n",
    "\n",
    "Predpokladom na úspešné zvládnutie tohto cvičenia je pripravené programátorské prostredie s `tensorflow`om a `keras`om. Ak dané nástroje ste si ešte nenainštalovali, urobte tak podľa [tohto návodu](https://github.com/ianmagyar/dl-course/blob/master/labs/00%20-%20Setting-up-tensorflow.md). V priebehu cvičenia sa takisto oboznámime so základnými krokmi pri trénovaní hlbokých neurónových sietí.\n",
    "\n",
    "Kostru riešenia nájdete [tu](resources/lab04/lab04.py) alebo môžete pracovať priamo v tomto jupyter notebooku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Načítanie potrebných knižníc\n",
    "\n",
    "V prvom kroku načítame všetky potrebné moduly pre definovanie, trénovanie a vyhodnotenie siete. Dokumentáciu príslušných tried a metód nájdete [tu pre keras](https://keras.io/api/) a [tu pre scikit-learn](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predspracovanie údajov\n",
    "\n",
    "Prvým dôležitým krokom pri vývoja hlbokých riešení je načítanie a predspracovanie datasetu. Samotné predpsracovanie v sebe zahŕňa hneď niekoľko úloh, ako výber príznakov, normalizácia hodnôt, vektorizácia vstupov a výstupov alebo rozdelenie datasetu na trénovaciu a testovaciu (prípadne validačnú) množinu.\n",
    "\n",
    "V skripte nájdete príkaz na načítanie datasetu MNIST, ktorý obsahuje čiernobiele obrázky rukou písaných číslic (viď nižšie) od 0 po 9. Dataset vieme načítať priamo z knižnice `keras` cez funkciu `load_data`. Pomocné premenné `img_height` a `img_width` obsahujú rozmery každého obrázka a slúžia pre opätovné použitie týchto hodnôt neskôr v kóde.\n",
    "\n",
    "![MNIST dataset](https://miro.medium.com/max/495/0*94t_5cPF9mvBj20z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# we set the image sizes for further use\n",
    "img_height, img_width = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako vidíte v kóde, pri načítaní datasetu hneď ho rozdelíme aj na trénovaciu a testovaciu množinu.\n",
    "\n",
    "Ďalším krokom je normalizácia hodnôt. V prvom rade potrebujeme vstupné čísla reprezentujúce pixely normalizovať na interval 0-1 a ešte je potrebné upraviť dimenzionalitu `numpy` polí, ktoré obsahujú vstupné dáta. Polia `x_train` a `x_test` rozšírime o jednu dimenziu s rozmerom 1, teda každú hodnotu \"zabalíme\" do wrappera, aby sa nám lepšie pracovalo s nimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the training and testing input sets - add a channel\n",
    "# normalize input data to interval 0 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výstupné dáta `y_train` a `y_test` majú formu poľa celých čísel od 0 po 9 (reprezentujúce číslice), keďže ale neriešime regresiu ale klasifikáciu, potrebujeme ich pretransformovať do podoby vektorov. Táto vektorová reprezentácia sa používa veľmi často pri spracovaní kategórií a reťazcov neurónovými sieťami. Na programovú realizáciu existuje niekoľko možností, my dnes použijeme funkciu `to_categorical` z knižnice `keras` ([dokumentácia](https://keras.io/api/utils/python_utils/#to_categorical-function))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (vectorize) output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poznámka:** v niektorých prípadoch dokážete reťazce nahradiť jednoduchými číslami, takýto spôsob ale predpokladá, že čísla, ktoré sú blízko sebe vyjadrujú koncepty, ktoré sú veľmi podobné. Napríklad, ak máme stĺpec s hodnotami *low*, *middle*, *high*, tieto hodnoty vieme nahradiť číslami 1, 2 a 3. Rovnaký spôsob ale nemôžeme použiť s hodnotami napríklad značky auta: *Škoda* (1), *Audi* (2), *Lada* (3), pretože neurónová sieť by predpokladala, že Lada (3) je viac podobná Audi (2) ako Škodovke (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definícia modelu\n",
    "\n",
    "Našu hlbokú sieť zadefinujeme pomocou knižnice `keras`, ktorá ponúka vysokoúrovňové API na prácu s neurónovými sieťami. Na vytvorenie modelov máme dve možnosti: najprv vytvoríme objekt modelu a postupne doňho pridávame vrstvy, alebo najprv zadefinujeme a pospájame vrstvy a následne zadefinujeme model pomocou vstupnej a výstupnej vrstvy. Na cvičení použijeme prvý spôsob, sekvenčný model už máte v kóde vytvorený ([dokumentácia](https://keras.io/api/models/sequential/#sequential-class)). Pridajte do tohto modelu vrstvy podľa diagramu nižšie. V konvolučných a plne prepojených vrstvách použite aktiváciu `relu` a vo výstupnej vrstve `softmax`. Konvolučné kernely majú rozmer *5x5*, *pooling* kernely *2x2*.\n",
    "\n",
    "![](resources/lab04/lenet-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, add layers based on the diagram\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vašu implementáciu si môžete skontrolovať pomocou funkcie `summary` ([dokumentácia](https://keras.io/api/models/model/#summary-method))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veľkým rozdielom medzi `tensorflow`om a `pycharm`om je potreba modely skompilovať pred trénovaním. Slúži na to metóda `compile` ([dokumentácia](https://keras.io/api/models/model_training_apis/#compile-method)). Pomocou nej **skompilujte model zadaním parametrov nasledovne**:\n",
    "\n",
    "- chybová funkcia - *categorical crossentropy*\n",
    "- optimalizátor - *Adam*\n",
    "- metriky - presnosť (*accuracy*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trénovanie modelu\n",
    "\n",
    "Po skompilovaní modelu ho môžeme natrénovať, na čo slúži metóda `fit` ([dokumentácia](https://keras.io/api/models/model_training_apis/#fit-method)). Jej hlavné parametre sú:\n",
    "\n",
    "- trénovací vstup\n",
    "- trénovací výstup\n",
    "- `epochs` - počet epoch\n",
    "- `batch_size` - počet príkladov v jednej dávke (pred aktualizáciou parametrov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vyhodnotenie siete\n",
    "\n",
    "Vyhodnotenie siete pozostáva z dvoch základných úloh: testovanie a vyhodnotenie. Pre testovanie musíme získať výstupy k vstupným hodnotám z trénovacej množiny pomocou metódy `predict` ([dokumentácia](https://keras.io/api/models/model_training_apis/#predict-method)). Alternatívne môžete na jednoduché vyhodnotenie použiť metódu `evaluate` ([dokumentácia](https://keras.io/api/models/model_training_apis/#evaluate-method))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ďalej porovnáme ozajstné výstupy s očakávanými. Keďže výstup má vektorovú reprezentáciu, potrebujeme zistiť pozíciu kde sa nachádza najväčšia hodnota vo vektore. V tomto nám pomôže knižnica `numpy`, ktorú sme zatiaľ nepoužili explicitne, ale podporuje všetky už použité knižnice. Jedná sa o efektívne a optimalizované riešenie práce s poľami.\n",
    "\n",
    "Pre vyhodnotenie našej siete použijeme konfúznu maticu. Konfúzna matica je tabuľková reprezentácia, kde v riadkoch máme očakávané triedy a v stĺpcoch vypočítané (predikované). V bunkách tabuľky sú uložené počty príkladov klasifikované v danej kombinácii očakávanej a predikovanej triedy. Ideálny klasifikátor bude mať všetky hodnoty po hlavnej diagonále (ďalšie informácie nájdete na [wikipédii](https://en.wikipedia.org/wiki/Confusion_matrix))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z konfúznej matici potom vieme vypočítať ďalšie metriky, ako presnosť (*accuracy*), návratnosť (*recall*) a precizita (*precision*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presnosť popisuje samotný klasifikátor a vypočíta sa nasledovne:\n",
    "\n",
    "$ACC = \\frac{TP + TN}{P + N}$\n",
    "\n",
    "kde $TP + TN$ je suma správne klasifikovaných príkladov (na hlavnej diagonále) a $P + N$ je počet všetkých príkladov.\n",
    "\n",
    "Návratnosť a precizita popisujú klasifikátor pre danú triedu, vypočítajú sa nasledovne:\n",
    "\n",
    "$REC = \\frac{TP}{P}$\n",
    "\n",
    "$PREC = \\frac{TP}{TP + FP}$\n",
    "\n",
    "kde $TP$ je počet správne klasifikovaných príkladov z danej triedy, $P$ je počet príkadov z danej triedy v testovacej množine a $FP$ je počet príkladov z testovacej množiny nesprávne klasifikovaných do tejto triedy.\n",
    "\n",
    "Metóda `classification_report` vypočíta ešte hodnotu F1, ktorá je harmonický priemer návratnosti a precizity:\n",
    "\n",
    "$F1 = 2 \\cdot \\frac{REC \\cdot PREC}{REC + PREC}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doplňujúce úlohy\n",
    "\n",
    "1. Originálny model LeNet používal aktivačnú funkciu `tanh` a trénovanie pomocou jednoduchého backpropagation (*stochastic gradient descent*). Upravte váš model a porovnajte presnosť dvoch modelov.\n",
    "2. Pri trénovaní sa často používa aj validačná množina, ktorá slúži na určenie včasného ukončenia trénovania a aby sme tak predišli pretrénovaniu modelu. Rozšírte vaše riešenie o použitie validačnej množiny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Použité zdroje\n",
    "\n",
    "- [LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86, no. 11 (1998): 2278-2324.](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&tag=1)\n",
    "- [LeNet on MNIST with Keras and Tensorflow in Python](https://github.com/matthewrenze/lenet-on-mnist-with-keras-and-tensorflow-in-python)\n",
    "- [TensorSpace Playground - LeNet](https://tensorspace.org/html/playground/lenet.html)\n",
    "- [Ukážka vizualizácie konvolučných sietí](lab04b-cnn-visualization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
