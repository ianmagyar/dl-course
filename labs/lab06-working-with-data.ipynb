{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6837c311-7d72-4e1b-9d2b-e8ed50eb3f43",
   "metadata": {},
   "source": [
    "# Práca s dátami\n",
    "\n",
    "Kvalita a množstvo dostupných dát je najkritickejšou zložkou pri úspešnom trénovaní hlbokých neurónových sietí, aj preto témou dnešného cvičenia je ich predspracovanie a príprava na použitie pri učení modelov. Pozrieme sa na možné prístupy pri práci s chýbajúcimi údajmi, na úpravu kategorických dát do podoby, ktorá je spracovateľná neurónovými sieťami, a na výber najdôležitejších príznakov pre trénovanie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb10b8-ca97-4b4c-8f15-7df3e4a18848",
   "metadata": {},
   "source": [
    "## Chýbajúce údaje\n",
    "\n",
    "V reálnych aplikáciách sa často stáva, že tréningovým príkladom chýbajú niektoré hodnoty z rôznych dôvodov. Môže sa jednať o chybu pri zbere dát, neaplikovateľnosť niektorých meraní alebo jednoducho nevyplnené miesta v dotazníku. Väčšina modelov nedokáže s chýbajúcimi hodnotami pracovať alebo tie by mohli priniesť nepredvídateľné výsledky, a preto je nevyhnutné zaoberať sa chýbajúcimi hodnotami ešte pred trénovaním."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5a589-6615-4748-bd08-0af94668b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "csv_data = '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7cfa58-b63e-4960-8f26-37c50b995cff",
   "metadata": {},
   "source": [
    "Možné prístupy pri spracovaní chýbajúcich hodnôt sú eliminácia príkladov a náhrada chýbajúcich hodnôt (napríklad priemerom atribútu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542c64f-325c-4e04-915d-6f0207cc4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91aa57-d11a-4c49-b4de-bfb8251dec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0)  # mazanie riadkov\n",
    "# df.dropna(axis=1)  # mazanie stĺpcov\n",
    "# df.dropna(how='all')  # mazanie iba riadkov v ktorých chýbajú všetky hodnoty\n",
    "# df.dropna(thresh=4)  # mazanie riadkov s menej ako 4 hodnotami\n",
    "# df.dropna(subset=['C'])  # mazanie riadkov, v ktorých chýba hodnota atribútu C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5583c5-ed71-40b3-949f-006d6b408358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data\n",
    "\n",
    "# alebo:\n",
    "# df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a866ceb-25f7-4de3-8921-7885402bac74",
   "metadata": {},
   "source": [
    "## Spracovanie kategorických údajov\n",
    "\n",
    "V datasetoch často nájdete aj nečíselné premenné, ktoré môžu nadobudnúť iba diskrétne hodnoty, reprezentujú teda kategorické dáta. Tie môžu byť ordinálne alebo nominálne. Ordinálne údaje môžeme zoradiť (napríklad veľkosť trička), kým nominálne nenaznačujú žiadne poradie (napríklad farba trička). Okrem toho často potrebujete transformovať aj atribút reprezentujúci triedu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c07c64-ad5d-40b9-ad8a-deb13b07737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([\n",
    "    ['green', 'M', 10.1, 'class2'],\n",
    "    ['red', 'L', 13.5, 'class1'],\n",
    "    ['blue', 'XL', 15.3, 'class2']])\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11064e-c017-44f8-9c0c-c38cd30d3285",
   "metadata": {},
   "source": [
    "V prípade ordinálnych príznakov ich môžeme nahradiť číselnými hodnotami z istej postupnosti, napríklad pre veľkosť tričiek môžeme použiť mapovanie: *M = 1, L = 2, XL = 3*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630a23b-18d9-4ead-a545-c86ca1fabddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mapping = { 'XL': 3, 'L': 2, 'M': 1 }\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175618cd-1df9-44f1-902a-56a6a347ecf2",
   "metadata": {},
   "source": [
    "**Poznámka:** Pre ordinálne hodnoty sa niekedy používa enkódovanie na základe prahovej hodnoty, napríklad veľkosť trička by sme vedeli reprezentovať ako dvojicu hodnôt, kde prvá vyjadruje, či veľkosť je väčšia ako M, kým druhá hodnota vyjadruje, či je tričko väčšie ako L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32201900-d9be-4fc8-af25-ad80d48ca426",
   "metadata": {},
   "source": [
    "Automatické priradenie hodnôt si môžeme ukázať na príklade značení tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752c18c-0600-404d-804e-bdbc6dcddc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    label: idx for idx, label in\n",
    "    enumerate(np.unique(df['classlabel']))\n",
    "}\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afdc6a-45da-4da1-9ead-6ba21dac9379",
   "metadata": {},
   "source": [
    "Pre úpravu očakávaných labelov sa často používa aj `LabelEncoder` zo `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e3183-0eff-472d-b439-a387d1ad99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c3fdc-7ffa-434d-a8ce-901f3e2307b0",
   "metadata": {},
   "source": [
    "Pre nominálne hodnoty je potrebné vykonať ešte jeden krok, a to **one-hot encoding**, kde nominálne príznaky reprezentujeme ako vektor s jedinou hodnotou 1 (na ostatných pozíciách máme 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55073d-7994-46b0-b54b-1205ad7d76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = df[['color', 'size', 'price']].values\n",
    "color_ohe = OneHotEncoder()\n",
    "color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1f72e-9f42-49f8-8a7f-9646a378bccc",
   "metadata": {},
   "source": [
    "**Poznámka:** Dávajte si pozor na to, že one-hot encoding často vytvára kolineárne vektory, kde jednotlivé atribúty silne korelujú, práve preto sa niekedy vynechá jedna hodnota zo získaného vektora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b0467-60bd-4134-bd36-45c753edba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)  # get_dummies works on string values only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8a1d0-4330-4f07-b15d-604212a797e8",
   "metadata": {},
   "source": [
    "## Škálovanie hodnôt\n",
    "\n",
    "Cieľom škálovania je upraviť reprezentáciu príznakov tak, aby sa hodnoty jednotlivých príznakov pohybovali v intervale 0-1 so štandardnou deviáciou 1. To umožní lepšiu klasifikáciu príkladov. Ako už bolo vysvetlené na predchádzajúcich cvičeniach, takúto normalizáciu vždy vykonávame iba na základe trénovacích údajov. Najčastejšie prístupy sú Min-Max škálovanie a škálovanie na základe priemeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467d599-9c9c-41a8-b9fe-f3220c78bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "df_wine.columns = [\n",
    "    'Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash',\n",
    "    'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
    "    'Proanthocyanins', 'Color intensity', 'Hue',\n",
    "    'OD280/OD315 of diluted wines', 'Proline'\n",
    "]\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2b1ca-1de6-468c-9ccd-43b96f88f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26322a89-e560-4e39-93ef-9048b8e9d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "\n",
    "print(X_train_norm[:5])\n",
    "print(X_test_norm[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b78d1-c6f7-42f6-badf-843da49961ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)\n",
    "\n",
    "print(X_train_std[:5])\n",
    "print(X_test_std[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a248d-76db-4d9d-9cac-93db9c809765",
   "metadata": {},
   "source": [
    "## Výber príznakov\n",
    "\n",
    "Ak model má k dispozícii veľa príznakov pre jednotlivé príklady, môže dôjsť k pretrénovaniu, keďže sa model príliš sústreďuje na priame súvislosti medzi niektorým atribútom a výsledkom predikcie v trénovacích dátach, ktoré však nemusia platiť pre testovacie dáta. Možným riešením pretrénovania je samozrejme regularizácia, ktorú sme preberali na predošlom cvičení, v niektorých prípadoch však môže pomôcť aj zmenšenie dimenzionality údajov a to konkrétne dvomi prístupmi: výberom príznakov a extrakciou príznakov. Pod výberom príznakov rozumieme selekciu niektorých atribútov, ktoré neskôr použijeme pri predikcii, pričom cieľom extrakcie je vyťažiť zo všetkých príznakov charakteristické črty, ktoré lepšie popisujú príklady. Výsledkom výberu je teda zmenšený priestor príznakov, pričom extrakcia nám vytvorí úplne nový priestor príznakov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf61b6-cc0c-4d39-97bf-46ec2554d04a",
   "metadata": {},
   "source": [
    "### Sekvenčný výber príznakov\n",
    "\n",
    "Cieľom sekvenčného výberu príznakov je zmenšiť dimenzionalitu príznakového priestoru na najrelevantnejšie príznaky. Tým sa odstráni šum, zvýšia sa generalizačné schopnosti modelu a zlepší sa výpočtová efektivita. Príkladom je algoritmu **sequential backward selection**, ktorý primárne slúži na zlepšenie efektivity. Algoritmus po jednom odstráni príznaky, až kým nedosiahneme ich cieľový počet. Robí to na základe kriteriálnej funkcie *J*, ktorú chceme minimalizovať (napríklad rozdiel medzi výkonom klasifikátora pred a po odstránení príznaku)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82a452-b22f-4059-94f0-15dfdbcb37a9",
   "metadata": {},
   "source": [
    "### Vyhodnotenie dôležitosti príznakov\n",
    "\n",
    "Pre zníženie počtu príznakov pri lineárne separabilnom probléme môžeme používať kombináciu L1 regularizácie a lineárnej regresie. V prípade nelinearít však táto metóda je nepoužiteľná, vtedy môžeme použiť napríklad **random forest** na určenie dôležitosti jednotlivých príznakov. Dôležitosť sa vypočíta z priemernej miery entropie jednotlivých rozhodovacích stromov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2a39c-47f8-40d5-b798-ec96534fa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_labels = df_wine.columns[1:]\n",
    "forest = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.title('Feature importance')\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6badbb-e89d-48f5-b926-7420314b7076",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "**Principal Component Analysis** je základnou metódou extrakcie príznakov, ktorá okrem redukcie dimenzionality príznakového priestoru sa používa aj na dátovú analýzu a odstránenie šumu. PCA dokáže odhaliť súvislosti a vzory v dátach na základe korelácie medzi príznakmi. Takáto analýza odhalí smer najväčšej variancie.\n",
    "\n",
    "PCA reálne vygeneruje transformačnú maticu $d \\times k$, ktorú vieme použiť na mapovanie *d*-dimenzionálneho príkladu do *k* dimenzií. Postup pri analýze je nasledovný:\n",
    "\n",
    "1. **Štandardizujte *d*-dimenzionálny dataset**.\n",
    "2. **Vytvorte maticu kovariancie**.\n",
    "3. **Rozložte maticu kovariancie na vlastné vektory a vlastné čísla**.\n",
    "4. **Usporiadajte vlastné čísla zostupne, aby ste zoradili príslušné vlastné vektory**.\n",
    "5. **Vyberte *k* vlastných vektorov**, ktoré zodpovedajú *k* najväčším vlastným číslam, kde *k* je dimenzionalita novej podmnožiny príznakov.\n",
    "6. **Vytvorte projekčnú maticu W z top *k* vlastných vektorov**.\n",
    "7. **Transformujte *d*-dimenzionálny vstupný dataset *X* pomocou projekčnej matice *W*** na získanie novej *k*-dimenzionálnej podmnožiny príznakov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14478c2f-928b-4365-b95f-a559932e3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b728b3-9753-4210-8edb-7802d07f05f4",
   "metadata": {},
   "source": [
    "Príklad transformácie klasifikačnej úlohy pomocou PCA:\n",
    "\n",
    "![](lab06/pca1.jpg)\n",
    "\n",
    "![](lab06/pca2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114348c1-9efd-41f0-9bdd-e7f0491039f8",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "Kým PCA hľadá osi maximálnej variancie, cieľom **Linear Discriminant Analysis** je nájsť podmnožinu (extrahovaných) príznakov, ktorá maximalizuje separabilitu tried. LDA tým pádom na rozdiel od PCA predstavuje kontrolovaný algoritmus, ktorý berie do úvahy cieľové triedy. LDA tiež pracuje s niekoľkými predpokladmi, ako normálne rozdelenie tried, identické kovariančné matice tried a štatistická nezávislosť príkladov. Tieto predpoklady však nie sú striktné, algoritmus pomerne dobre funguje aj keď niektorý z nich je porušený.\n",
    "\n",
    "1. **Štandardizujte *d*-dimenzionálny dataset** (kde *d* je počet príznakov).\n",
    "2. **Pre každú triedu vypočítajte *d*-dimenzionálny priemerný vektor**. (kontrolovaná časť algoritmu)\n",
    "3. **Vytvorte maticu rozptylu medzi triedami, $S_B$, a maticu rozptylu v rámci tried, $S_W$**.\n",
    "4. **Vypočítajte vlastné vektory a príslušné vlastné čísla matice $S_W^{-1} S_B$**.\n",
    "5. **Usporiadajte vlastné čísla zostupne, aby ste zoradili príslušné vlastné vektory**.\n",
    "6. **Vyberte *k* vlastných vektorov**, ktoré zodpovedajú *k* najväčším vlastným číslam, a vytvorte transformačnú maticu $W$ s rozmermi *d×k*; vlastné vektory tvoria stĺpce tejto matice.\n",
    "7. **Projekcia príkladov na novú množinu vlastností** pomocou transformačnej matice $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735d46f-e127-455e-8cae-ca414691c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b65c29-a227-45f1-97de-d1200bd8569d",
   "metadata": {},
   "source": [
    "## Krížová validácia\n",
    "\n",
    "V *k*-násobnej krížovej validácii náhodne rozdelíme trénovacie dáta na *k* častí bez náhrady. Z týchto častí sa *k – 1* použije na trénovanie modelu (*training fold*) a jedna časť (*test fold*) na vyhodnotenie výkonnosti. Tento postup sa opakuje *k*-krát, čím získame *k* modelov a k odhadov výkonnosti. Výkonnosť modelov je vypočítaná z priemeru týchto modelov. Ak máte dostatočné množstvo údajov, vyhodnotenie môžete robiť na rovnakých testovacích dátach, ktoré nevstupujú do krížovej validácie, alebo môžete použiť výkonnosť modelov na testovacích foldoch. Krížová validácia sa v druhom prípade zvyčajne používa na optimalizáciu hyperparametrov.\n",
    "\n",
    "Najčastejšie sa používa 10-násobná krížová validácia, keďže tá sa ukázala byť najreprezentatívnejšie a najviac balansuje bias-variance tradeoff. Čím je dataset menší, tím viac foldov by sme mali použiť, pri väčších datasetoch sa často používa *k = 5*. Ak máte nevybalansovaný dataset, môžete používať stratifikovanú krížovú validáciu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3b2bc-768e-44a7-9a83-e740f30e5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    # train your model: fit(X_train[train], y_train[train]) \n",
    "    # evaluate your model: score(X_train[test], y_train[test])\n",
    "    pass\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
