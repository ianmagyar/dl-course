{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb320d06",
   "metadata": {},
   "source": [
    "# Spracovanie prirodzeného jazyka (zatiaľ bez DL)\n",
    "\n",
    "Veľké množstvo textových dát môže slúžiť ako zdroj cenných informácií a práve preto sa v posledných rokoch **spracovanie prirodzeného jazyka** (*natural language processing* - NLP) stalo dôležitou podoblasťou strojového učenia. Na nasledujúcich cvičeniach sa pozrieme na základné princípy NLP a úlohy špecifické pre texty. Na dnešnom cvičení si ukážeme príklad takzvanej analýzy sentimentu, teda kategorizácie textu podľa prístupu a názoru autora.\n",
    "\n",
    "Dnes zatiaľ nebudeme na to používať neurónové siete, keďže je dôležité, aby ste pochopili postupnosť krokov pri spracovaní textu. Na ďalšom cvičení si ukážeme, ako špeciálny typ neurónových sietí, rekurentná NS, pomôže pri tejto úlohe. Náš výlet v oblasti NLP uzavrieme pohľadom na attention mechanizmy a transformery ako State of the Art pre NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f95f70",
   "metadata": {},
   "source": [
    "Na dnešnom cvičení budeme pracovať s datasetom IMDb, ktorý obsahuje 50 000 recenzií filmov, ktoré sú zaradené do dvoch kategórií: pozitívnych (viac ako 6 hviezdičiek) a negatívnych (menej ako 5 hviezdičiek). Po stiahnutí datasetu nás čakajú úlohy ako:\n",
    "\n",
    "1. predspracovanie dát;\n",
    "2. vektorizácia textových údajov;\n",
    "3. trénovanie modelu strojového učenia pre klasifikáciu;\n",
    "4. práca s veľkými textovými dátami;\n",
    "5. estimácia obsahu textu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce4b91",
   "metadata": {},
   "source": [
    "## 1. Predspracovanie dát\n",
    "\n",
    "[Dataset si môžete stiahnuť z tohto odkazu](http://ai.stanford.edu/~amaas/data/sentiment/), následne si rozbaľte súbor (cca 80 MB).\n",
    "\n",
    "Môžete si všimnúť, že dáta sú rozdelené do dvoch adresárov na trénovanie a testovanie, a v rámci týchto priečinkov nájdeme veľa súborov. Pre pohodlnejšiu prácu si tieto dáta nakopírujeme do jedného CSV súboru (proces môže trvať niekoľko minút). Ak sa vám nechce čakať na výsledky, [môžete si stiahnuť hotový CSV súbor](lab07/movie_data.csv) (cca 64 MB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cefda31",
   "metadata": {},
   "source": [
    "Rozbalenie môžete urobiť priamo v Pythone, čo by mohlo byť rýchlejšie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43801724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"lab07/aclImdb_v1.tar.gz\", 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "# pip install pyprind\n",
    "import pyprind\n",
    "\n",
    "BASEPATH = \"lab07/aclImdb\"\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "df = pd.DataFrame()\n",
    "for subdir in ('test', 'train'):\n",
    "    for cat in ('pos', 'neg'):\n",
    "        path = os.path.join(BASEPATH, subdir, cat)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[cat]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17e528",
   "metadata": {},
   "source": [
    "Pred ukladaním načítaných dát si ich pomiešame v náhodnom poradí, a následne ich uložíme do CSV súboru (ak ste si stiahli hotový súbor, tieto kroky môžete vynechať)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(\"movie_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97f409",
   "metadata": {},
   "source": [
    "Následne si dataset znova načítame (tento krok už potrebujete urobiť), trošku upravíme, a skontrolujeme množstvo a obsah dát:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2429fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_data.csv\", encoding='utf-8')\n",
    "# krok potrebny na niektorych pocitacoch\n",
    "# df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb3463",
   "metadata": {},
   "source": [
    "## 2. Vektorizácia textových údajov\n",
    "\n",
    "Neurónové siete, ale aj ďalšie algoritmy strojového učenia boli navrhnuté tak, aby dokázali pracovať s číselnými dátami, čo sa nedá povedať o texte. Práve preto v prípade textu a iných kategorických údajov je potrebné tieto údaje pretransformovať do číselnej reprezentácie. V tomto kroku použijeme prístup **bag-of-words**, ktorý každému slovu pridelí jedinečný príznakový vektor. Tento proces sa uskutoční v dvoch krokoch:\n",
    "\n",
    "1. vytvoríme si zásobu jedinečných tokenov - napríklad slov - zo všetkých dokumentov.\n",
    "2. zostrojíme príznakový vektor z každého dokumentu, kde vektor obsahuje informáciu o tom, koľkokrát sa dané slovo vyskytuje v danom dokumente.\n",
    "\n",
    "Je jasné, že väčšina hodnôt vo vektoroch bude 0, t.j. vektory budú **sparse**, čo je presne to, čo potrebujeme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4d119",
   "metadata": {},
   "source": [
    "### 2.1. Generovanie príznakových vektorov\n",
    "\n",
    "Pre generovanie vektorov použijeme knižnicu `scikit-learn`, ktorá je súčasťou inštalácie Anaconda. Proces si ukážeme na jednoduchých dátach, a neskôr ho aplikujeme na náš dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['Roses are red',\n",
    "                 'Violets are blue',\n",
    "                 'Roses are read, violets are blue, wine costs less than dinner for two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337e0e3",
   "metadata": {},
   "source": [
    "Následne si môžeme skontrolovať a analyzovať obsah vygenerovaných vektorov:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d34e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71256e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbff54",
   "metadata": {},
   "source": [
    "Vygenerovaná zásoba slov reprezentuje index daného slova vo vektorovej reprezentácii a vektory obsahujú počet výskytov daného slova vo vete. Tento počet nazývame aj ako **raw term frequencies**: *tf(t, d)* - počet výskytov výrazu *t* v dokumente *d*. Na poradí týchto výrazov nám nezáleží, ich poradie je odvodené z indexov zásoby slov (zvyčajne podľa abecedy).\n",
    "\n",
    "**Poznámka**: V našom bag-of-words modeli sme použili 1-gram (unigram) model, ale existujú aj iné reprezentácie, kde sa jeden výraz skladá z viacerých tokenov, napríklad bigram: *roses are*, *are red*. Rôzne úlohy vyžadujú rôznu aritu reprezentácie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b1800",
   "metadata": {},
   "source": [
    "Nevýhoda TF reprezentácie je, že sa niektoré slová často vyskytujú v príkladoch oboch typov (pozitívne a negatívne), a práve preto zvyčajne nemajú veľkú výpovednú hodnotu pre klasifikáciu. Namiesto toho teda, aby sme brali do úvahy ich surovú početnosť v dátach, môžeme použiť techniku **term frequency-inverse document frequency**: $$tf{\\text -}idf(t, d) = tf(t, d) \\times idf(t, d),$$\n",
    "\n",
    "kde\n",
    "\n",
    "$$\n",
    "idf(t,d) = log\\frac{n_{d}}{1 + df(d, t)}.\n",
    "$$\n",
    "\n",
    "$n_{d}$ je celkový počet dokumentov, a $df(d, t)$ je počet dokumentov *d*, ktoré obsahujú výraz *t*. Konkrétne implementácie v knižnici `scikit-learn` fungujú s menšími zmenami, ale to nás nemusí zaujímať.\n",
    "\n",
    "Našu reprezentáciu vieme prekonvertovať do TF-IDF formy pomocou kódu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b263acf",
   "metadata": {},
   "source": [
    "### 2.2 Čistenie dát\n",
    "\n",
    "Skutočné textové dáta často obsahujú špeciálne znaky, ktoré nemajú žiadnu výpovednú hodnotu, a práve preto by bolo vhodné ich vymazať. Zoberte príklad z jednej recenzie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fde44",
   "metadata": {},
   "source": [
    "Text obsahuje HTML markupy, ako aj interpunkčné znamienka. V niektorých prípadoch síce interpunkcia zohráva veľkú rolu pri vyhodnocovaní textu, v našom prípade ju však nepotrebujeme, takže odstránime ju spolu s HTML tagmi. K tomu ešte spracujeme aj smajlíky ako bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessor(df.loc[0, 'review'][-50:]))\n",
    "print(preprocessor(\"</a>This :) is :( a test :-)!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac60e5",
   "metadata": {},
   "source": [
    "Predspracovanie teraz už vieme aplikovať na našich skutočných dátach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d498089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514469e1",
   "metadata": {},
   "source": [
    "### 2.3. Tokenizácia dokumentov\n",
    "\n",
    "Prvý krok pri spracovaní textu je jeho rozdelenie na menšie bloky, tzv. tokeny, ktoré väčšinou sú slová. Základným spôsobom rozdelenia do slov je rozdelenie viet podľa medzier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "print(tokenizer(\"runners like running and thus they run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379a296",
   "metadata": {},
   "source": [
    "### 2.4. Stemming (a lematizácia)\n",
    "\n",
    "Ako môžete vidieť na príklade vyššie, niektoré slová (ako *running* a *run*, sčasti aj *runners*) reprezentujú podobný koncept, práve preto je zbytočné ich mať viackrát v zásobe slov. Takisto by sme potrebovali spracovať množné čísla a previesť slová do singulárneho tvaru. Tento proces sa nazýva **stemming** a existuje niekoľko algoritmov na jeho realizáciu, jeden zo základných je **Porter stemming**, ktorý je dostupný v knižnici Natural Language Toolkit (súčasťou Anacondy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer_porter(\"runners like running and thus they run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc975b6",
   "metadata": {},
   "source": [
    "Na vyššom príklade vidíte efekt stemmingu, ktorý ale nie je dokonalý proces (napríklad *thus* nesprávne zjednodušil na tvar *thu*). Druhý podobný proces - lematizácia - vám vždy vráti základný slovníkový tvar slova, je však výpočtovo náročnejší aj keď zvyčajne dáva lepšie výsledky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17042098",
   "metadata": {},
   "source": [
    "### 2.5. Odstránenie stop slov\n",
    "\n",
    "Stop slová sú slová, ktoré sa často vyskytujú vo všetkých textoch a práve preto ich nemá veľký zmysel spracovať, keďže nemajú veľkú informačnú hodnotu pre klasifikáciu alebo inú úlohu strojového učenia. Síce *tf-idf* čiastočne eliminuje potrebu odstránenia takýchto slov, keďže zmenší dôležitosť často sa opakujúcich slov, v niektorých prípadoch je dôležité tieto slová odstrániť, na čo existujú hotové množiny pre rôzne jazyky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86481cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07618b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print([w for w in tokenizer_porter(\"a runner likes running and runs a lot\") if w not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac45e7",
   "metadata": {},
   "source": [
    "## 3. Trénovanie klasifikačného modelu\n",
    "\n",
    "Na dnešnom cvičení síce ešte nebudeme používať neurónové siete, proces trénovania klasifikačného modelu si však ukážeme pomocou logistickej regresie. Pri trénovaní použijeme aj optimalizáciu hyperparametrov pomocou `sklearn`. Najprv si ale pripravíme trénovacie a testovacie dáta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb33d01",
   "metadata": {},
   "source": [
    "Následne si natrénujeme model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "small_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    }\n",
    "]\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           verbose=2, n_jobs=1)  # n_jobs=-1 for parallel processing\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952191d6",
   "metadata": {},
   "source": [
    "Najúspešnejšie nastavenie parametrov a príslušné výsledky nájdeme pomocou kódu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bed585",
   "metadata": {},
   "source": [
    "Môžeme to overiť aj na testovacích dátach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69196c3f",
   "metadata": {},
   "source": [
    "## 4. Práca s veľkými textovými dátami\n",
    "\n",
    "V predošlom kroku ste si mohli všimnúť, že optimalizácia hyperparametrov môže trvať pomerne dlho pri predspracovaní údajov ako tokenizácia a následný stemming. Ak máte veľký dataset, kľudne sa môže stať, že sa vám celý dataset ani nezmestí do pamäti počítača, čo spôsobí zlyhanie takéhoto vyhľadávania.\n",
    "\n",
    "Podobné problémy pri neurónových sieťach sa riešia minibatch trénovaním, a obdobný prístup existuje aj pre iné modely, tzv. **out-of-core learning**, kde model trénujeme iba parciálne na menšom počte príkladov pomocou funkcie `partial_fit`.\n",
    "\n",
    "V prvom kroku si zadefinujeme nový tokenizer s odstránením stop slov:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baab296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def new_tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9c86f",
   "metadata": {},
   "source": [
    "Ďalej si zadefinujeme generátor, ktorý nám vráti jednotlivé dokumenty z celkového datasetu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(stream_docs(path=\"movie_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca2eb4",
   "metadata": {},
   "source": [
    "Ďalšia funkcia nám dodá jeden minibatch trénovacích údajov:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b978605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889472a3",
   "metadata": {},
   "source": [
    "Pri vektorizácii teraz už nemôžeme používať `CountVectorizer` alebo `TfidfVectorizer`, keďže tieto metódy vyžadujú informáciu o celkovom počte výskytov jednotlivých slov v datasete. Namiesto toho použijeme iný typ vektorizácie, konkrétne `HashingVectorizer`, ktorý je nezávislý od dát:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0319bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,  # vector size - estimate\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=new_tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path=\"movie_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d920375",
   "metadata": {},
   "source": [
    "Konečne môžeme začať aj trénovanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d827e",
   "metadata": {},
   "source": [
    "Po úspešnom trénovaní môžeme skontrolovať aj trénovaciu presnosť:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b22a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbb39a",
   "metadata": {},
   "source": [
    "Finálna presnosť môže byť trošku nižšia ako v predošlom prípade, avšak out-of-core trénovanie je omnoho rýchlejšie a menej zaťažuje pamäť počítača.\n",
    "\n",
    "Keďže \"testovaciu\" množinu sme použili na validáciu, ak sme už spokojní s výsledkami, môžeme tieto dáta použiť aj na trénovanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192c056",
   "metadata": {},
   "source": [
    "## 5. Estimácia obsahu textu\n",
    "\n",
    "Estimácia obsahu textu nám môže pomôcť pri zhlukovaní neolabelovaných textových dát, kde sa snažíme prideliť lable jednotlivým textom na základe ich obsahu. Jedným z možných algoritmov na riešenie takéhoto problému je **latent Dirichlet allocation**, alebo LDA, ktorá je založená na Bayesovej inferencii. Jej cieľom je nájsť slová, ktoré sa často vyskytujú spoločne vo viacerých dokumentoch, a tak definujú tému, resp. kategóriu. Jej vstupom je bag-of-words model, z ktorého vygeneruje dve matice: document-to-topic a word-to-topic, pričom ich znásobením dostaneme naspäť pôvodný text s čo najmenšou chybou. Hyperparametrom LDA je počet hľadaných kategórií."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f855c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\", encoding='utf-8')\n",
    "# krok potrebny na niektorych pocitacoch\n",
    "# df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b89d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,  # ignore words with a frequency above 10%\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561e28e",
   "metadata": {},
   "source": [
    "Po trénovaní môžeme vizualizovať najčastejšie slová v jednotlivých kategóriách:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                    [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cbece",
   "metadata": {},
   "source": [
    "Ukážkové príklady pre kategóriu *horror*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c2542",
   "metadata": {},
   "source": [
    "## Použité zdroje\n",
    "\n",
    "* **IMDb dataset**: Maas, Andrew, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. \"Learning word vectors for sentiment analysis.\" In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 142-150. 2011.\n",
    "* **Porter stemmer:** Porter, Martin F. \"An algorithm for suffix stripping.\" Program 14, no. 3 (1980): 130-137.\n",
    "* **Natural Language Toolkit:** https://www.nltk.org\n",
    "* **Latent Dirichlet allocation:** Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3, no. Jan (2003): 993-1022.\n",
    "* Raschka, Sebastian, Yuxi Hayden Liu, Vahid Mirjalili, and Dmytro Dzhulgakov. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
