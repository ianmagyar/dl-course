{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e982eea",
   "metadata": {},
   "source": [
    "# Modelovanie sekvenčných dát pomocou rekurentných neurónových sietí\n",
    "\n",
    "Doterajšie neurónové siete, ktoré sme trénovali, boli všetky založené na viacvrstvovom perceptróne, ktorý sa dokáže učiť iba v prípade, ak trénovacie dáta sú nezávislé a pochádzajú z rovnakej distribúcie. To znamená, že pri trénovaní nezáleží na poradí ich použitia. Takýto prístup sme vedeli rozšíriť aj na obrazové dáta, kde sme použili konvolúciu pre extrahovanie príznakov. Čo by sa ale stalo, keby sme chceli spracovať video pomocou neurónovej siete?\n",
    "\n",
    "Video sa síce skladá zo snímok, tie však nie sú nezávislé a na ich poradí celkom jednoznačne záleží. Na dnešnom cvičení sa pozrieme na ďalší typ neurónových sietí - rekurentných NN - ktoré dokážu pracovať s takýmito **sekvenčnými dátami**. Použijeme pritom ako vstup text, a ukážeme si dva základné prípady použitia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98070f1a",
   "metadata": {},
   "source": [
    "## Sekvenčné dáta\n",
    "\n",
    "Za sekvenčné dáta považujeme ľubovoľnú sadu dát, pri ktorej záleží na poradí prvkov, teda dataset vieme popísať iba v kontexte nejakej postupnosti, všeobecne: $(x^{(1)}, x^{(2)}, ..., x^{(T)})$. Táto postupnosť môže byť časová (časové rady), alebo sa môže jednať o bežnú sekvenciu údajov, ako napríklad pri prirodzenom jazyku, alebo pri sekvenovaní genómu, prípadne DNA. Typickým príkladom časových radov sú hodnoty akcií na burze.\n",
    "\n",
    "Pri sekvenčných dátach výstup je ovplyvňovaný nielen posledným údajom, ale aj postupnosťou, ktorá predchádzala poslednému vstupu. Predstavte si pohyb guličky na obraze; na predikciu ďalšej pozície vám vo väčšine prípadov nestačí poznať jej poslednú polohu, ale potrebujete vedieť aj predošlé polohy, aby ste vedeli urobiť presnú predikciu jej pohybu.\n",
    "\n",
    "Na druhej strane, nie v každom prípade existuje jedinečný výstup pre každý vstupný údaj, vo všeobecnosti poznáme hneď niekoľko typov úloh pri práci so sekvenčnými údajmi:\n",
    "\n",
    "* **many-to-one** - vstupom je sekvencia, ale výstupom je jediný vektor s istou dĺžkou, prípadne iba jedno číslo. Príkladom je analýza sentimentu v texte.\n",
    "* **one-to-many** - vstupom je jediná hodnota, a my chceme z nej vygenerovať postupnosť údajov. Príkladom je generovanie popisky k obrazu.\n",
    "* **many-to-many** - vstup aj výstup je sekvencia. Túto kategóriu vieme ďalej deliť podľa toho, či sa výstup generuje so skĺzom alebo okamžite. Synchronizovaná predikcia je napríklad klasifikácia videa, kde olabelujeme každú snímku. Predikcia so skĺzom môže zahŕňať strojový preklad, kde preklad textu nemôžeme začať riešiť okamžite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffe91f-6fda-43f3-8258-9ca26cd3c113",
   "metadata": {},
   "source": [
    "## Spracovanie prirodzeného jazyka\n",
    "\n",
    "Veľké množstvo textových dát môže slúžiť ako zdroj cenných informácií a práve preto sa v posledných rokoch **spracovanie prirodzeného jazyka** (*natural language processing* - NLP) stalo dôležitou podoblasťou strojového učenia. Na nasledujúcich cvičeniach sa pozrieme na základné princípy NLP a úlohy špecifické pre texty. Na dnešnom cvičení si ukážeme príklad takzvanej analýzy sentimentu, teda kategorizácie textu podľa prístupu a názoru autora. Budeme pracovať s datasetom IMDb, ktorý obsahuje 50 000 recenzií filmov, ktoré sú zaradené do dvoch kategórií: pozitívnych (viac ako 6 hviezdičiek) a negatívnych (menej ako 5 hviezdičiek). Po stiahnutí datasetu nás čakajú úlohy ako:\n",
    "\n",
    "1. predspracovanie dát;\n",
    "2. vektorizácia textových údajov;\n",
    "3. trénovanie modelu strojového učenia pre klasifikáciu;\n",
    "4. práca s veľkými textovými dátami;\n",
    "5. estimácia obsahu textu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d2799-d3f0-4c6a-8184-bc88af1c410d",
   "metadata": {},
   "source": [
    "### 1. Predspracovanie dát\n",
    "\n",
    "[Dataset si môžete stiahnuť z tohto odkazu](http://ai.stanford.edu/~amaas/data/sentiment/), následne si rozbaľte súbor (cca 80 MB).\n",
    "\n",
    "Môžete si všimnúť, že dáta sú rozdelené do dvoch adresárov na trénovanie a testovanie, a v rámci týchto priečinkov nájdeme veľa súborov. Pre pohodlnejšiu prácu si tieto dáta nakopírujeme do jedného CSV súboru (proces môže trvať niekoľko minút). Ak sa vám nechce čakať na výsledky, [môžete si stiahnuť hotový CSV súbor](lab07/movie_data.csv) (cca 64 MB).\n",
    "\n",
    "Rozbalenie môžete urobiť priamo v Pythone, čo by mohlo byť rýchlejšie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec09f3-0029-4f5c-8606-a44b29799091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"lab07/aclImdb_v1.tar.gz\", 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c94a83-cf62-4df8-9601-214389156c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "# pip install pyprind\n",
    "import pyprind\n",
    "\n",
    "BASEPATH = \"lab07/aclImdb\"\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "df = pd.DataFrame()\n",
    "for subdir in ('test', 'train'):\n",
    "    for cat in ('pos', 'neg'):\n",
    "        path = os.path.join(BASEPATH, subdir, cat)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[cat]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf3a1e-7aa6-47e8-9a74-db6908a4f71c",
   "metadata": {},
   "source": [
    "Pred ukladaním načítaných dát si ich pomiešame v náhodnom poradí, a následne ich uložíme do CSV súboru (ak ste si stiahli hotový súbor, tieto kroky môžete vynechať)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8a0a2-3933-4dd3-9595-6fde884bd12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(\"movie_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4384b-7565-48db-b440-2f3102b9a4dc",
   "metadata": {},
   "source": [
    "Následne si dataset znova načítame (tento krok už potrebujete urobiť), trošku upravíme, a skontrolujeme množstvo a obsah dát:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197de5dd-3815-421f-93a6-70d94f11b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_data.csv\", encoding='utf-8')\n",
    "# krok potrebny na niektorych pocitacoch\n",
    "# df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22851fcb-fcc2-4586-a405-e589a1d46d13",
   "metadata": {},
   "source": [
    "### 2. Vektorizácia textových údajov\n",
    "\n",
    "Neurónové siete, ale aj ďalšie algoritmy strojového učenia boli navrhnuté tak, aby dokázali pracovať s číselnými dátami, čo sa nedá povedať o texte. Práve preto v prípade textu a iných kategorických údajov je potrebné tieto údaje pretransformovať do číselnej reprezentácie. V tomto kroku použijeme prístup **bag-of-words**, ktorý každému slovu pridelí jedinečný príznakový vektor. Tento proces sa uskutoční v dvoch krokoch:\n",
    "\n",
    "1. vytvoríme si zásobu jedinečných tokenov - napríklad slov - zo všetkých dokumentov.\n",
    "2. zostrojíme príznakový vektor z každého dokumentu, kde vektor obsahuje informáciu o tom, koľkokrát sa dané slovo vyskytuje v danom dokumente.\n",
    "\n",
    "Je jasné, že väčšina hodnôt vo vektoroch bude 0, t.j. vektory budú **sparse**, čo je presne to, čo potrebujeme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0c8c1-67ea-4db4-bd52-209224b8a596",
   "metadata": {},
   "source": [
    "#### 2.1. Generovanie príznakových vektorov\n",
    "\n",
    "Pre generovanie vektorov použijeme knižnicu `scikit-learn`, ktorá je súčasťou inštalácie Anaconda. Proces si ukážeme na jednoduchých dátach, a neskôr ho aplikujeme na náš dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1010910-09db-4278-9bfc-a5b393a91def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['Roses are red',\n",
    "                 'Violets are blue',\n",
    "                 'Roses are read, violets are blue, wine costs less than dinner for two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c46fd0-ac3f-4a94-a36c-68c9fc89045e",
   "metadata": {},
   "source": [
    "Následne si môžeme skontrolovať a analyzovať obsah vygenerovaných vektorov:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27a0c7-1206-42d9-acf9-ca9967525912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7e2ef-4b45-474e-8019-b56088927566",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34de6e-695e-4b55-9253-32b1ad719ae6",
   "metadata": {},
   "source": [
    "Vygenerovaná zásoba slov reprezentuje index daného slova vo vektorovej reprezentácii a vektory obsahujú počet výskytov daného slova vo vete. Tento počet nazývame aj ako **raw term frequencies**: *tf(t, d)* - počet výskytov výrazu *t* v dokumente *d*. Na poradí týchto výrazov nám nezáleží, ich poradie je odvodené z indexov zásoby slov (zvyčajne podľa abecedy).\n",
    "\n",
    "**Poznámka**: V našom bag-of-words modeli sme použili 1-gram (unigram) model, ale existujú aj iné reprezentácie, kde sa jeden výraz skladá z viacerých tokenov, napríklad bigram: *roses are*, *are red*. Rôzne úlohy vyžadujú rôznu aritu reprezentácie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79963139-6fc2-41e2-80bd-e74a30e51a3c",
   "metadata": {},
   "source": [
    "Nevýhoda TF reprezentácie je, že sa niektoré slová často vyskytujú v príkladoch oboch typov (pozitívne a negatívne), a práve preto zvyčajne nemajú veľkú výpovednú hodnotu pre klasifikáciu. Namiesto toho teda, aby sme brali do úvahy ich surovú početnosť v dátach, môžeme použiť techniku **term frequency-inverse document frequency**: $$tf{\\text -}idf(t, d) = tf(t, d) \\times idf(t, d),$$\n",
    "\n",
    "kde\n",
    "\n",
    "$$\n",
    "idf(t,d) = log\\frac{n_{d}}{1 + df(d, t)}.\n",
    "$$\n",
    "\n",
    "$n_{d}$ je celkový počet dokumentov, a $df(d, t)$ je počet dokumentov *d*, ktoré obsahujú výraz *t*. Konkrétne implementácie v knižnici `scikit-learn` fungujú s menšími zmenami, ale to nás nemusí zaujímať.\n",
    "\n",
    "Našu reprezentáciu vieme prekonvertovať do TF-IDF formy pomocou kódu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3934b4-d538-480c-8002-35967fe38aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f92cd4-9e5d-4b91-8e4e-2b845348009e",
   "metadata": {},
   "source": [
    "#### 2.2 Čistenie dát\n",
    "\n",
    "Skutočné textové dáta často obsahujú špeciálne znaky, ktoré nemajú žiadnu výpovednú hodnotu, a práve preto by bolo vhodné ich vymazať. Zoberte príklad z jednej recenzie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f6a9a-763f-4f02-9f27-23a842e50db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba36b1-fa0f-485b-9d3a-da15e34a93b9",
   "metadata": {},
   "source": [
    "Text obsahuje HTML markupy, ako aj interpunkčné znamienka. V niektorých prípadoch síce interpunkcia zohráva veľkú rolu pri vyhodnocovaní textu, v našom prípade ju však nepotrebujeme, takže odstránime ju spolu s HTML tagmi. K tomu ešte spracujeme aj smajlíky ako bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c027c-4883-46ea-b169-60320caf398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfb3ae-4b7b-4794-a506-9e3e61441f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessor(df.loc[0, 'review'][-50:]))\n",
    "print(preprocessor(\"</a>This :) is :( a test :-)!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce776f1e-79b5-493c-b911-f30804b37b54",
   "metadata": {},
   "source": [
    "Predspracovanie teraz už vieme aplikovať na našich skutočných dátach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ccdd9-26a1-482f-9a70-d1a707e82bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a003a7-06a5-40e0-b45d-edab79f6da3e",
   "metadata": {},
   "source": [
    "#### 2.3. Tokenizácia dokumentov\n",
    "\n",
    "Prvý krok pri spracovaní textu je jeho rozdelenie na menšie bloky, tzv. tokeny, ktoré väčšinou sú slová. Základným spôsobom rozdelenia do slov je rozdelenie viet podľa medzier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6724fe-e389-4a80-bc79-5ae14fa77d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "print(tokenizer(\"runners like running and thus they run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7a939-7d38-4208-ba62-ba7dc9090112",
   "metadata": {},
   "source": [
    "#### 2.4. Stemming (a lematizácia)\n",
    "\n",
    "Ako môžete vidieť na príklade vyššie, niektoré slová (ako *running* a *run*, sčasti aj *runners*) reprezentujú podobný koncept, práve preto je zbytočné ich mať viackrát v zásobe slov. Takisto by sme potrebovali spracovať množné čísla a previesť slová do singulárneho tvaru. Tento proces sa nazýva **stemming** a existuje niekoľko algoritmov na jeho realizáciu, jeden zo základných je **Porter stemming**, ktorý je dostupný v knižnici Natural Language Toolkit (súčasťou Anacondy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47e71c-a6ae-4b78-bdb0-60e93f217701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer_porter(\"runners like running and thus they run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61167b8-731d-47f0-b876-85b8546375f6",
   "metadata": {},
   "source": [
    "Na vyššom príklade vidíte efekt stemmingu, ktorý ale nie je dokonalý proces (napríklad *thus* nesprávne zjednodušil na tvar *thu*). Druhý podobný proces - lematizácia - vám vždy vráti základný slovníkový tvar slova, je však výpočtovo náročnejší aj keď zvyčajne dáva lepšie výsledky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4607314-e0b1-4066-b6eb-040212e480ed",
   "metadata": {},
   "source": [
    "#### 2.5. Odstránenie stop slov\n",
    "\n",
    "Stop slová sú slová, ktoré sa často vyskytujú vo všetkých textoch a práve preto ich nemá veľký zmysel spracovať, keďže nemajú veľkú informačnú hodnotu pre klasifikáciu alebo inú úlohu strojového učenia. Síce *tf-idf* čiastočne eliminuje potrebu odstránenia takýchto slov, keďže zmenší dôležitosť často sa opakujúcich slov, v niektorých prípadoch je dôležité tieto slová odstrániť, na čo existujú hotové množiny pre rôzne jazyky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f8c1c-bfdd-4cc9-b85b-19a3e0c96a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6c7fe-5a38-4f89-bd14-292709d4dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print([w for w in tokenizer_porter(\"a runner likes running and runs a lot\") if w not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad994df-16a7-4679-a670-513073e56a57",
   "metadata": {},
   "source": [
    "#### 2.6. Príprava na trénovanie\n",
    "\n",
    "Pred tým, než použijeme neurónové siete, proces trénovania klasifikačného modelu si ukážeme pomocou logistickej regresie. Pri trénovaní použijeme aj optimalizáciu hyperparametrov pomocou `sklearn`. Najprv si ale pripravíme trénovacie a testovacie dáta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178abf40-9f61-4f9a-9d3e-e7f43efc1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72d0b3-3839-4493-924b-0824d3b98cb7",
   "metadata": {},
   "source": [
    "Následne si natrénujeme model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e0d8b-cd71-4c64-9d05-5ca52d3ac02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "small_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    }\n",
    "]\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           verbose=2, n_jobs=1)  # n_jobs=-1 for parallel processing\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd5624-40b1-43db-900c-7968fe9d6e9b",
   "metadata": {},
   "source": [
    "Najúspešnejšie nastavenie parametrov a príslušné výsledky nájdeme pomocou kódu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86f0af-ac19-4e80-a7b6-2dab8755844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5725269-f91f-4e0a-a997-32a8a926f162",
   "metadata": {},
   "source": [
    "Môžeme to overiť aj na testovacích dátach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659f248-4f35-46a2-b283-e904ec272fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abfa86",
   "metadata": {},
   "source": [
    "## Rekurentné neurónové siete\n",
    "\n",
    "V doterajších dopredných štruktúrach sme mali jasnú postupnosť krokov, kde na základe vstupu $x$ sa vygenerovala najprv aktivácia skrytej vrstvy $h$, a následne výstup siete $o$. Rekurentné siete zakomponujú aj časový údaj, a vstup do skrytého bloku je rozšírený o predošlý stav siete, ako je to znázornené na obrázku nižšie.\n",
    "\n",
    "![](lab07/rnn-scheme.jpg)\n",
    "\n",
    "Spôsob výpočtu výstupu je úplne rovnaký ako v predošlých prípadoch, až na to, že máme tu časom rozšírenú štruktúru:\n",
    "\n",
    "![](lab07/rnn-activation.jpg)\n",
    "\n",
    "Pracujeme teda s tromi maticami váh: $w_{xh}$ - váhy smerujúce od vstupu do skrytej vrstvy; $w_{hh}$ - váhy rekurentného bloku; $w_{ho}$ - váhy smerujúce od rekurentného bloku do výstupu. Výpočty potom prebiehajú nasledovne (všetky hodnoty sú reálne matice alebo vektory):\n",
    "\n",
    "1. najprv sa vypočíta predaktivácia rekurentného bloku\n",
    "    $$sum_{h}^{(t)} = w_{xh} x^{(t)} + w_{hh} h^{(t-1)} + b_h$$\n",
    "2. v ďalšom kroku vypočítame aktiváciu rekurentného bloku\n",
    "    $$act_{h}^{(t)} = act( sum_{h}^{(t)} ) = act( w_{xh} x^{(t)} + w_{hh} h^{(t-1)} + b_h )$$\n",
    "3. výstup siete sa vypočíta podľa štandardných procesov\n",
    "    $$act_{o}^{(t)} = act( w_{ho} h^{(t)} + b_o )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03677ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True) \n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e52f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh    \n",
    "    print('   Hidden          :', ht.detach().numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.detach().numpy())\n",
    "    print('   RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464df4d8",
   "metadata": {},
   "source": [
    "Pri výpočte chyby musíme zohľadniť časovú závislosť:\n",
    "\n",
    "$$L=\\sum_{t=1}^{T} L^{(t)}$$\n",
    "\n",
    "Pri učení pomocou backpropagation potom:\n",
    "\n",
    "$$\\frac{\\delta L^{(t)}}{\\delta \\textbf{W}_{hh}} = \\frac{\\delta L^{(t)}}{\\delta \\textbf{o}^{(t)}} \\times \\frac{\\delta \\textbf{o}^{(t)}}{\\delta \\textbf{h}^{(t)}} \\times \\left ( \\sum_{k=1}^{t} \\frac{\\delta \\textbf{h}^{(t)}}{\\delta \\textbf{h}^{(k)}} \\times \\frac{\\delta \\textbf{h}^{(k)}}{\\delta \\textbf{W}_{(hh)}} \\right )$$\n",
    "\n",
    "pričom\n",
    "\n",
    "$$\\frac{\\delta \\textbf{h}^{(t)}}{\\delta \\textbf{h}^{(k)}} = \\prod_{i=k+1}^{t} \\frac{\\delta \\textbf{h}^{(i)}}{\\delta \\textbf{h}^{(i-1)}}$$\n",
    "\n",
    "Ďalšie variácie rekurentných blokov sú output-to-hidden, resp. output-to-output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52cbaa",
   "metadata": {},
   "source": [
    "Keďže pri backprope sa použije súčin jednotlivých komponentov, môžu nastať tri situácie:\n",
    "\n",
    "1. váhy v rekurentnom bloku sú menšie ako jeden - vanishing gradient;\n",
    "2. váhy v rekurentnom bloku sú vyššie ako jeden - exploding gradient;\n",
    "3. váhy v rekurentnom bloku sú jeden - ideálny prípad.\n",
    "\n",
    "Okrem udržiavania váh v okolí 1, ďalšie možnosti riešenia gradientového problému sú:\n",
    "\n",
    "* gradient clipping - nastavíme minimálnu resp. maximálnu hodnotu gradientu, mimo ktorého intervalu gradient vynulujeme;\n",
    "* truncated backpropagation through time - nastavíme maximálny počet časových cyklov, ktoré ovplyvňujú gradient;\n",
    "* LSTM siete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfec4b",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory siete\n",
    "\n",
    "Základom LSTM sietí je **memory cell**, ktorá nahrádza skrytú vrstvu štandardnej RNN. Každá bunka obsahuje rekurentnú hranu s váhou 1, hodnoty spojené s touto hranou reprezentujú stav bunky. Celková štruktúra bunky potom vyzerá nasledovne:\n",
    "\n",
    "![](lab07/lstm.jpg)\n",
    "\n",
    "Ako aj blokový model naznačuje, stav bunky ($c^{(t)}$) je upravovaný zložitejším procesom, a skladá sa z niekoľkých krokov (niekedy nazývané *gate*-y). Tieto gate-y určujú, kedy ktorá informácia má byť zahodená. Bunka okrem toho naďalej má aj skrytý stav ($h^{(t)}$). Jednotlivé gate-y potom fungujú nasledovne:\n",
    "\n",
    "* **forget gate** - zabezpečuje, aby si bunka mohla reinicializovať svoju pamäť, tým pádom hodnota nenarastá nekonečne (určuje, ktorá hodnota sa zapamätá).\n",
    "$$f_{t} = \\sigma \\left ( \\textbf{W}_{xf} \\textbf{x}^{(t)} + \\textbf{W}_{hf} \\textbf{h}^{(t-1)} + \\textbf{b}_{f} \\right )$$\n",
    "* **input gate** - aktualizuje stav bunky spolu s kandidátnou hodnotou:\n",
    "$$i_{t} = \\sigma \\left ( \\textbf{W}_{xi} \\textbf{x}^{(t)} + \\textbf{W}_{hi} \\textbf{h}^{(t-1)} + \\textbf{b}_{i} \\right )$$\n",
    "$$\\tilde{C}_{t} = \\tanh \\left ( \\textbf{W}_{xc} \\textbf{x}^{(t)} + \\textbf{W}_{hc} \\textbf{h}^{(t-1)} + \\textbf{b}_{c} \\right )$$\n",
    "\n",
    "z toho\n",
    "\n",
    "$$C^{(t)} = (C^{(t-1)} \\odot f_{t}) \\oplus (i_t \\odot \\tilde{C}_{t})$$\n",
    "\n",
    "* **output gate** - rieši aktualizáciu hodnôt skrytých neurónov:\n",
    "$$o_{t} = \\sigma \\left ( \\textbf{W}_{xo} \\textbf{x}^{(t)} + \\textbf{W}_{ho} \\textbf{h}^{(t-1)} + \\textbf{b}_{o} \\right )$$\n",
    "\n",
    "z toho\n",
    "\n",
    "$$h^{(t)} = o_t \\odot \\tanh (C^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82f877",
   "metadata": {},
   "source": [
    "## Príklady\n",
    "\n",
    "Príklady na využitie rekurentných sietí nájdete [tu pre analýzu sentimentu](https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb) (rovnaká úloha ako na cvičení) a [tu pre modelovanie jazyka a generovanie textu](https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part3.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37696b09",
   "metadata": {},
   "source": [
    "## Použité zdroje\n",
    "\n",
    "* Blog, Andrej Karpathy. \"The Unreasonable Effectiveness of Recurrent Neural Networks.\" URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ dated May 21 (2015): 31.\n",
    "* Werbos, Paul J. \"Backpropagation through time: what it does and how to do it.\" Proceedings of the IEEE 78, no. 10 (1990): 1550-1560.\n",
    "* Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. \"On the difficulty of training recurrent neural networks.\" In International conference on machine learning, pp. 1310-1318. Pmlr, 2013.\n",
    "* Memory, Long Short-Term. \"Long short-term memory.\" Neural computation 9, no. 8 (2010): 1735-1780.\n",
    "* Gers, Felix A., Jürgen Schmidhuber, and Fred Cummins. \"Learning to forget: Continual prediction with LSTM.\" Neural computation 12, no. 10 (2000): 2451-2471.\n",
    "* Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \"Empirical evaluation of gated recurrent neural networks on sequence modeling.\" arXiv preprint arXiv:1412.3555 (2014).\n",
    "\n",
    "* Raschka, Sebastian, Yuxi Hayden Liu, Vahid Mirjalili, and Dmytro Dzhulgakov. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022. Kapitola 15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
